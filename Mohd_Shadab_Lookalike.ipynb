{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a54b1c-98f3-4fb0-8d41-ee282e74735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CustomerID        CustomerName         Region  SignupDate\n",
      "0      C0001    Lawrence Carroll  South America  2022-07-10\n",
      "1      C0002      Elizabeth Lutz           Asia  2022-02-13\n",
      "2      C0003      Michael Rivera  South America  2024-03-07\n",
      "3      C0004  Kathleen Rodriguez  South America  2022-10-09\n",
      "4      C0005         Laura Weber           Asia  2022-08-15\n",
      "  ProductID              ProductName     Category   Price\n",
      "0      P001     ActiveWear Biography        Books  169.30\n",
      "1      P002    ActiveWear Smartwatch  Electronics  346.30\n",
      "2      P003  ComfortLiving Biography        Books   44.12\n",
      "3      P004            BookWorld Rug   Home Decor   95.69\n",
      "4      P005          TechPro T-Shirt     Clothing  429.31\n",
      "  TransactionID CustomerID ProductID      TransactionDate  Quantity  \\\n",
      "0        T00001      C0199      P067  2024-08-25 12:38:23         1   \n",
      "1        T00112      C0146      P067  2024-05-27 22:23:54         1   \n",
      "2        T00166      C0127      P067  2024-04-25 07:38:55         1   \n",
      "3        T00272      C0087      P067  2024-03-26 22:55:37         2   \n",
      "4        T00363      C0070      P067  2024-03-21 15:10:10         3   \n",
      "\n",
      "   TotalValue   Price  \n",
      "0      300.68  300.68  \n",
      "1      300.68  300.68  \n",
      "2      300.68  300.68  \n",
      "3      601.36  300.68  \n",
      "4      902.04  300.68  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "customers_df = pd.read_csv('Customers.csv')\n",
    "products_df = pd.read_csv('Products.csv')\n",
    "transactions_df = pd.read_csv('Transactions.csv')\n",
    "\n",
    "# Check the first few rows of each dataframe to understand the data structure\n",
    "print(customers_df.head())\n",
    "print(products_df.head())\n",
    "print(transactions_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d31b62-3f77-4e94-8ee9-db7934adff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TransactionID CustomerID ProductID      TransactionDate  Quantity  \\\n",
      "0        T00001      C0199      P067  2024-08-25 12:38:23         1   \n",
      "1        T00112      C0146      P067  2024-05-27 22:23:54         1   \n",
      "2        T00166      C0127      P067  2024-04-25 07:38:55         1   \n",
      "3        T00272      C0087      P067  2024-03-26 22:55:37         2   \n",
      "4        T00363      C0070      P067  2024-03-21 15:10:10         3   \n",
      "\n",
      "   TotalValue  Price_x     CustomerName         Region  SignupDate  \\\n",
      "0      300.68   300.68   Andrea Jenkins         Europe  2022-12-03   \n",
      "1      300.68   300.68  Brittany Harvey           Asia  2024-09-04   \n",
      "2      300.68   300.68  Kathryn Stevens         Europe  2024-04-04   \n",
      "3      601.36   300.68  Travis Campbell  South America  2024-04-11   \n",
      "4      902.04   300.68    Timothy Perez         Europe  2022-03-15   \n",
      "\n",
      "                       ProductName     Category  Price_y  \n",
      "0  ComfortLiving Bluetooth Speaker  Electronics   300.68  \n",
      "1  ComfortLiving Bluetooth Speaker  Electronics   300.68  \n",
      "2  ComfortLiving Bluetooth Speaker  Electronics   300.68  \n",
      "3  ComfortLiving Bluetooth Speaker  Electronics   300.68  \n",
      "4  ComfortLiving Bluetooth Speaker  Electronics   300.68  \n"
     ]
    }
   ],
   "source": [
    "# Merge dataframes\n",
    "data = pd.merge(transactions_df, customers_df, on=\"CustomerID\", how=\"left\")\n",
    "data = pd.merge(data, products_df, on=\"ProductID\", how=\"left\")\n",
    "\n",
    "# Check the merged data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5c397b-1277-4561-81a1-c48a1f053498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CustomerID  TotalValue  Quantity  ProductID  ProductName\n",
      "0      C0001     3354.52        12          5            5\n",
      "1      C0002     1862.74        10          4            4\n",
      "2      C0003     2725.38        14          4            4\n",
      "3      C0004     5354.88        23          8            8\n",
      "4      C0005     2034.24         7          3            3\n"
     ]
    }
   ],
   "source": [
    "# Aggregate data by CustomerID (summarize transaction data)\n",
    "customer_transactions = data.groupby('CustomerID').agg({\n",
    "    'TotalValue': 'sum',               # Total spending by customer\n",
    "    'Quantity': 'sum',                 # Total quantity of products purchased\n",
    "    'ProductID': pd.Series.nunique,    # Number of different products purchased\n",
    "    'ProductName': pd.Series.nunique   # Number of unique products purchased\n",
    "}).reset_index()\n",
    "\n",
    "# Check the aggregated data\n",
    "print(customer_transactions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c0db8af-2080-4228-b6a8-61168919991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CustomerID        CustomerName         Region  SignupDate  Region_Asia  \\\n",
      "0      C0001    Lawrence Carroll  South America  2022-07-10          0.0   \n",
      "1      C0002      Elizabeth Lutz           Asia  2022-02-13          1.0   \n",
      "2      C0003      Michael Rivera  South America  2024-03-07          0.0   \n",
      "3      C0004  Kathleen Rodriguez  South America  2022-10-09          0.0   \n",
      "4      C0005         Laura Weber           Asia  2022-08-15          1.0   \n",
      "\n",
      "   Region_Europe  Region_North America  Region_South America  \n",
      "0            0.0                   0.0                   1.0  \n",
      "1            0.0                   0.0                   0.0  \n",
      "2            0.0                   0.0                   1.0  \n",
      "3            0.0                   0.0                   1.0  \n",
      "4            0.0                   0.0                   0.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-Hot Encoding for the 'Region' column in customers_df\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use sparse_output instead of sparse\n",
    "region_encoded = encoder.fit_transform(customers_df[['Region']])\n",
    "\n",
    "# Create a dataframe for the encoded region features\n",
    "region_df = pd.DataFrame(region_encoded, columns=encoder.get_feature_names_out())\n",
    "customers_df = pd.concat([customers_df, region_df], axis=1)\n",
    "\n",
    "# Check the result of encoding\n",
    "print(customers_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13a7984-dda4-4bab-97fc-f4f0107f89f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CustomerID  TotalValue  Quantity  ProductID  ProductName\n",
      "0      C0001   -0.061701 -0.122033          5            5\n",
      "1      C0002   -0.877744 -0.448000          4            4\n",
      "2      C0003   -0.405857  0.203934          4            4\n",
      "3      C0004    1.032547  1.670787          8            8\n",
      "4      C0005   -0.783929 -0.936951          3            3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize numerical features like TotalValue and Quantity\n",
    "scaler = StandardScaler()\n",
    "customer_transactions[['TotalValue', 'Quantity']] = scaler.fit_transform(\n",
    "    customer_transactions[['TotalValue', 'Quantity']]\n",
    ")\n",
    "\n",
    "# Check the scaled features\n",
    "print(customer_transactions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86c5c5c8-41ad-4547-b966-8f9fee84f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming customers_df and products_df are already loaded\n",
    "\n",
    "# Perform One-Hot Encoding for categorical variables like Region and Category\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# One-Hot encode 'Region' (from customers_df) and 'Category' (from products_df)\n",
    "region_encoded = encoder.fit_transform(customers_df[['Region']])\n",
    "category_encoded = encoder.fit_transform(products_df[['Category']])\n",
    "\n",
    "# Convert these encoded arrays back to dataframes\n",
    "region_df = pd.DataFrame(region_encoded, columns=encoder.get_feature_names_out())  # Removed the input feature names\n",
    "category_df = pd.DataFrame(category_encoded, columns=encoder.get_feature_names_out())  # Removed the input feature names\n",
    "\n",
    "# Merge these encoded columns into their respective dataframes\n",
    "customers_df = pd.concat([customers_df, region_df], axis=1)\n",
    "products_df = pd.concat([products_df, category_df], axis=1)\n",
    "\n",
    "# Now merge all dataframes together (transactions, customer, product data)\n",
    "data = pd.merge(transactions_df, customers_df, on=\"CustomerID\", how=\"left\")\n",
    "data = pd.merge(data, products_df, on=\"ProductID\", how=\"left\")\n",
    "\n",
    "# Aggregate the data to get customer-level information (total spending, product counts, etc.)\n",
    "customer_transactions = data.groupby('CustomerID').agg({\n",
    "    'TotalValue': 'sum', \n",
    "    'Quantity': 'sum',\n",
    "    'ProductID': pd.Series.nunique,\n",
    "}).reset_index()\n",
    "\n",
    "# Merge back the customer and product features for similarity computation\n",
    "features = customer_transactions.drop(['CustomerID'], axis=1)  # Drop non-numeric columns\n",
    "features = pd.concat([features, region_df, category_df], axis=1)  # Add encoded categorical features\n",
    "\n",
    "# Handle missing values (you can choose one of these methods)\n",
    "features = features.fillna(0)  # Or use features.fillna(features.mean()) or features.dropna()\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "features[['TotalValue', 'Quantity']] = scaler.fit_transform(features[['TotalValue', 'Quantity']])\n",
    "\n",
    "# Now calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(features)\n",
    "\n",
    "# Check the shape of the similarity matrix\n",
    "print(similarity_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "090d118f-9f40-452e-8ab8-77514994d65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200)\n",
      "('C0001', ['C0107', 'C0003', 'C0048'], [0.9809128736900199, 0.9920189297704356, 0.9986540040379928])\n"
     ]
    }
   ],
   "source": [
    "# Assuming the previous steps are correct\n",
    "\n",
    "# Make sure the 'features' DataFrame doesn't contain NaN values before computing similarity\n",
    "features = features.fillna(0)  # or fillna(features.mean()) if you want to fill with the mean\n",
    "\n",
    "# Standardize the numerical features again if needed\n",
    "scaler = StandardScaler()\n",
    "features[['TotalValue', 'Quantity']] = scaler.fit_transform(features[['TotalValue', 'Quantity']])\n",
    "\n",
    "# Now calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(features)\n",
    "\n",
    "# Ensure similarity_matrix is correctly calculated and available\n",
    "print(similarity_matrix.shape)  # Check if the matrix was generated\n",
    "\n",
    "# Assuming get_top_3_lookalikes is defined, call it for the first 20 customers\n",
    "lookalikes = []\n",
    "for customer_id in customers_df['CustomerID'].iloc[:20]:\n",
    "    lookalikes.extend(get_top_3_lookalikes(customer_id, similarity_matrix))\n",
    "\n",
    "# Check the results for the first customer\n",
    "print(lookalikes[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e03d992-07f9-4a60-bde1-f19eaeb20b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_3_lookalikes(customer_id, similarity_matrix):\n",
    "    # Find the index of the customer in the data\n",
    "    customer_index = customers_df[customers_df['CustomerID'] == customer_id].index[0]\n",
    "    \n",
    "    # Get the similarity scores for the customer\n",
    "    similarity_scores = similarity_matrix[customer_index]\n",
    "    \n",
    "    # Get the indices of the top 3 most similar customers\n",
    "    top_3_indices = similarity_scores.argsort()[-4:-1][::-1]  # Exclude the customer itself (index 0)\n",
    "    \n",
    "    # Retrieve the CustomerIDs of the most similar customers\n",
    "    top_3_customers = customers_df.iloc[top_3_indices]['CustomerID'].values\n",
    "    return top_3_customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa329339-7492-482a-803d-203844744185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CustomerID             Lookalikes  \\\n",
      "0      C0001  [C0107, C0003, C0048]   \n",
      "1      C0002  [C0056, C0027, C0088]   \n",
      "2      C0003  [C0174, C0048, C0001]   \n",
      "3      C0004  [C0102, C0169, C0113]   \n",
      "4      C0005  [C0146, C0159, C0092]   \n",
      "\n",
      "                                              Scores  \n",
      "0  [0.9809128736900199, 0.9920189297704356, 0.998...  \n",
      "1  [0.9801086612191465, 0.9934624308952046, 0.995...  \n",
      "2  [0.9678396355982816, 0.9864088743333652, 0.992...  \n",
      "3  [0.9897757651034574, 0.9915044528991657, 0.992...  \n",
      "4  [0.9451792332944101, 0.9579115659581428, 0.979...  \n"
     ]
    }
   ],
   "source": [
    "# Convert the results into a DataFrame\n",
    "lookalike_df = pd.DataFrame(lookalikes, columns=['CustomerID', 'Lookalikes', 'Scores'])\n",
    "\n",
    "# Save to CSV\n",
    "lookalike_df.to_csv('Lookalike.csv', index=False)\n",
    "\n",
    "# Check the output\n",
    "print(lookalike_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ba0c7-24ee-4f41-a81b-961f73c98b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
